{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Datalake Access Key configuration\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.cryptoanalyticslake.dfs.core.windows.net\",\n",
    "    dbutils.secrets.get(scope=\"key-vault-secret-scope\",key=\"cryptoanalyticslake-access-key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Day Month Year\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "today = datetime.utcnow()\n",
    "year = today.year\n",
    "month = today.month\n",
    "day = today.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive data load for all files from a day from every partition in the Event Hub Namespace\n",
    "sourcefolderpath = f\"abfss://crypto-quotes@cryptoanalyticslake.dfs.core.windows.net/ehns-quote-streams/eh-crypto-stream/*/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "print(sourcefolderpath)\n",
    "\n",
    "df = spark.read.option(\"recursiveFileLookup\",\"true\").option(\"header\",\"true\").format(\"avro\").load(sourcefolderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Body field from Binary to JSON \n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StringType, DoubleType, StructType, StructField\n",
    "\n",
    "sourceSchema = StructType([\n",
    "        StructField(\"Symbol\", StringType(), False),\n",
    "        StructField(\"Price\", DoubleType(), True),\n",
    "        StructField(\"PriceTimeStamp\", StringType(), True)])\n",
    "\n",
    "df = df.withColumn(\"StringBody\", col(\"Body\").cast(\"string\"))\n",
    "jsonOptions = {\"dateFormat\" : \"yyyy-MM-dd HH:mm:ss.SSS\"}\n",
    "df = df.withColumn(\"JsonBody\", from_json(df.StringBody, sourceSchema, jsonOptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattent he Body JSON field into columns of the DataFrame\n",
    "for c in df.schema[\"JsonBody\"].dataType:\n",
    "    df = df.withColumn(c.name, col(\"JsonBody.\" + c.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 0 priced assets\n",
    "df = df.filter(\"Price > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data\n",
    "df = df.sort(\"Symbol\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the meaningful columns for the export to Bronze data zone\n",
    "exportDF = df.select(\"Symbol\", \"Price\", \"PriceTimeStamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Price Date column\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "exportDF = exportDF.withColumn(\"PriceDate\", to_date(\"PriceTimeStamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the partquet file in the bronze crypto data zone\n",
    "sparkpartitionfolderpath = f\"abfss://crypto-bronze@cryptoanalyticslake.dfs.core.windows.net/quotes-by-day-spark-partition\"\n",
    "\n",
    "print(sparkpartitionfolderpath)\n",
    "\n",
    "exportDF.write.partitionBy(\"PriceDate\").mode(\"overwrite\").parquet(sparkpartitionfolderpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the partquet file in the bronze crypto data zone\n",
    "manualpartitionfolderpath = f\"abfss://crypto-bronze@cryptoanalyticslake.dfs.core.windows.net/quotes-by-day-manual-partition/{year}/{month:0>2d}/{day:0>2d}\"\n",
    "\n",
    "print(manualpartitionfolderpath)\n",
    "\n",
    "exportDF.write.mode(\"overwrite\").parquet(manualpartitionfolderpath)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
